TODO:
- [ ] explain what is the goal of the experiment; write this before you describe the algorithm;
- [ ] this point is still open each table should have a description; for each table and figure that you add, you need to mention which are your main findings/observations; see the Results section at the end of the page; there are many tables but very little description of those

# Introduction
## Collaborative Variational Autoencoder (CVAE) Model
The Collaborative Variational Autoencoder (CVAE) is a Bayesian probabilistic generative model that unifies the collaborative and content information through stochastic deep learning models and graphical models, leading to robust recommendation performance. It can jointly model the generation of item content while extracting the implicit relationships between items and users collaboratively.

![image](uploads/f7b2e4280f08aa53b1795b1735c02757/image.png)
The left part of the above figure is the proposed Collaborative Variational Autoencoder (CVAE); the right part is the zoom-in of the inference network and generation network in CVAE.

## Dataset
Since the CVAE model requires text modality data, we selected four datasets integrated into the Cornac framework and containing the text auxiliary information, and used an additional dataset, namely the MIND dataset. Below, we briefly introduce all the datasets we use in our experiments.

### Amazon clothing dataset and Amazon digital music dataset
The Amazon product dataset contains reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs) from Amazon, including 142.8 million reviews spanning May 1996 - July 2014. 

From the large Amazon product dataset collection, we choose to use the clothing dataset and the digital music dataset.

We use item description as auxiliary text information for the clothing dataset and the review text as auxiliary text information for the digital music dataset.

### CiteULike dataset
The CiteULike dataset is used in the original paper that proposed the CDL algorithm. The dataset was collected from CiteULike and Google Scholar. CiteULike allows users to create their own collections of articles. Data include abstracts, titles, and tags for each article, and other information like authors, groups, posting time, and keywords. The CiteULike dataset includes 2 sub-datasets - CiteULike-a and CiteULike-t. For these experiments, we use the CiteULike-a from Cornac.

The text information we used includes tile and abstract joined together into one document per item.

### Movienlens 10k dataset
The data was collected through the MovieLens website (movielens.umn.edu) during a seven-month period from September 19th, 1997 through April 22nd, 1998. This dataset consists of 100,000 ratings (1-5) from 943 users on 1682 movies. Each user has rated at least 20 movies. The dataset also contains demographic information of the users, such as age, gender, occupation, zip code.

For this dataset, we used the plot information of movies from the built-in module of Cornac as text modality.

### MIND dataset
The MIND dataset is the additional dataset besides the built-in Cornac datasets. MIcrosoft News Dataset (MIND) is a large-scale dataset for news recommendation research. It was collected from anonymized behavior logs of the Microsoft News website. MIND contains about 160k English news articles and more than 15 million impression logs generated by 1 million users. Every news article contains rich textual content, including title, abstract, body, category, and entities. Each impression log contains the click events, non-clicked events, and historical news click behaviors of this user before this impression. 

We use the news content as the auxiliary text information.

# Experiment
## Setup
1. Prepare dataset: load data in (item id, user id, rating) format
2. Split dataset: Normally we use 20% of the dataset as the test set.
3. Initialize model: we have developed a [script](https://gitlab.ifi.uzh.ch/ddis/Students/Projects/2023-diversity-framework/-/blob/algorithm/cornac/algorithm/cdl/cdl_mind.ipynb) to automate parameters changes, model training and results logging. Basically, we changed parameters like k,v to find the best parameters for both accuracy and diversity.
4. Define metrics to evaluate the model: we used all the Cornac built-in metrics to evaluate the accuracy of the model. Besides, we evaluate the diversity of the recommended items on these [addition metrics](https://gitlab.ifi.uzh.ch/ddis/Students/Projects/2023-diversity-framework/-/wikis/home#diversity-metrics).
5. Perform the experiment using the Cornac built-in `Experiment` module.

TODO:
- [ ] the link does not point to this algorithm

## Run model on different datasets with various parameters

TODO:
- [ ] this short section with the goal should be moved at the beginning, so that people that open the page understand from the beginning what is the page describing;

### Goal
The goal of these experiments is to find out, for each dataset, the optimal parameters of the model that allows us to recommend accurate and diverse items. We run the CVAE model on all available datasets, and evaluate its accuracy and diversity.

### Diversity metrics
To compute the calibration metric, we do not extra features extracted from the textual components of the items. To compute the fragmentation metric, however, we need to detect the stories in our data. We compute the [Calibration diversity metric](/ddis/Students/Projects/2023-diversity-framework/-/wikis/Calibration) and the [Fragmentation diversity metric](/ddis/Students/Projects/2023-diversity-framework/-/wikis/Fragmentation) for the MIND datasets, and calibration for the MovienLens dataset. The higher the value is, the more diverse the recommended items are.

The result of the experiment is shown in the table below. The first column is the name of the model, and the second column refers to the dataset. k, max_iter, lambda_v are some parameters we experiment with. And l2_reg, MAE, MSE, RMSE, AUC, F1, MAP, MRR, NCRR, NDCG, Precision, Recall are the accuracy metrics we used. The diversity metrics we used for evaluation are the calibration (category diversity) and fragmentation metrics.

| model | data | k   | max_iter | lambda_v | MAE    | MSE    | RMSE   | AUC    | F1     | MAP    | MRR    | NCRR   | NDCG   | Precision | Recall | category diversity  | story diversity    |
|-------|------|-----|----------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-----------|--------|---------------------|--------------------|
| CVAE  | mind | 10  | 10       | 0.001    | 0.6016 | 0.5876 | 0.7665 | 0.4972 | 0.0025 | 0.0049 | 0.0136 | 0.0049 | 0.0084 | 0.0014    | 0.0201 | 0.27888048635616400 | 1.290955915419160  |
| CVAE  | mind | 10  | 10       | 0.002    | 0.6016 | 0.5876 | 0.7665 | 0.4972 | 0.0025 | 0.0049 | 0.0136 | 0.0049 | 0.0084 | 0.0014    | 0.0201 | 0.27888048635616400 | 1.135169740480190  |
| CVAE  | mind | 10  | 10       | 0.005    | 0.6016 | 0.5876 | 0.7665 | 0.4972 | 0.0025 | 0.0049 | 0.0136 | 0.0049 | 0.0084 | 0.0014    | 0.0201 | 0.27888048635616400 | 1.2009823410394800 |
| CVAE  | mind | 10  | 10       | 0.01     | 0.6016 | 0.5876 | 0.7665 | 0.4972 | 0.0025 | 0.0049 | 0.0136 | 0.0049 | 0.0084 | 0.0014    | 0.0201 | 0.27888048635616400 | 1.1388379523670600 |
| CVAE  | mind | 50  | 10       | 0.001    | 0.5720 | 0.5343 | 0.7309 | 0.5558 | 0.0058 | 0.0061 | 0.0156 | 0.0058 | 0.0128 | 0.0034    | 0.0283 | 0.22415446114153600 | 1.2803051205209000 |
| CVAE  | mind | 50  | 10       | 0.002    | 0.5720 | 0.5343 | 0.7309 | 0.5558 | 0.0058 | 0.0061 | 0.0156 | 0.0058 | 0.0128 | 0.0034    | 0.0283 | 0.22415446114153600 | 1.29597593796229   |
| CVAE  | mind | 50  | 10       | 0.005    | 0.5720 | 0.5343 | 0.7309 | 0.5558 | 0.0058 | 0.0061 | 0.0156 | 0.0058 | 0.0128 | 0.0034    | 0.0283 | 0.22415446114153600 | 1.3223732162955100 |
| CVAE  | mind | 50  | 10       | 0.01     | 0.5720 | 0.5343 | 0.7309 | 0.5558 | 0.0058 | 0.0061 | 0.0156 | 0.0058 | 0.0128 | 0.0034    | 0.0283 | 0.22415446114153600 | 1.2866318374918900 |
| CVAE  | mind | 100 | 10       | 0.001    | 0.5517 | 0.5005 | 0.7074 | 0.5645 | 0.0050 | 0.0060 | 0.0141 | 0.0046 | 0.0112 | 0.0029    | 0.0267 | 0.28656999905171800 | 1.1776087820399100 |
| CVAE  | mind | 100 | 10       | 0.002    | 0.5517 | 0.5005 | 0.7074 | 0.5645 | 0.0050 | 0.0060 | 0.0141 | 0.0046 | 0.0112 | 0.0029    | 0.0267 | 0.28656999905171800 | 1.2603375175396000 |
| CVAE  | mind | 100 | 10       | 0.005    | 0.5517 | 0.5005 | 0.7074 | 0.5645 | 0.0050 | 0.0060 | 0.0141 | 0.0046 | 0.0112 | 0.0029    | 0.0267 | 0.28656999905171800 | 1.1816889894542400 |
| CVAE  | mind | 100 | 10       | 0.01     | 0.5517 | 0.5005 | 0.7074 | 0.5645 | 0.0050 | 0.0060 | 0.0141 | 0.0046 | 0.0112 | 0.0029    | 0.0267 | 0.28656999905171800 | 1.0808349068834900 |
| CVAE  | mind | 200 | 10       | 0.001    | 0.5434 | 0.4895 | 0.6996 | 0.6315 | 0.0066 | 0.0066 | 0.0139 | 0.0048 | 0.0155 | 0.0038    | 0.0451 | 0.5109829377062080  | 1.398895309228100  |
| CVAE  | mind | 200 | 10       | 0.002    | 0.5434 | 0.4895 | 0.6996 | 0.6315 | 0.0066 | 0.0066 | 0.0139 | 0.0048 | 0.0155 | 0.0038    | 0.0451 | 0.5109829377062080  | 1.2087962581728100 |
| CVAE  | mind | 200 | 10       | 0.005    | 0.5434 | 0.4895 | 0.6996 | 0.6315 | 0.0066 | 0.0066 | 0.0139 | 0.0048 | 0.0155 | 0.0038    | 0.0451 | 0.5109829377062080  | 1.1573286566319400 |
| CVAE  | mind | 200 | 10       | 0.01     | 0.5434 | 0.4895 | 0.6996 | 0.6315 | 0.0066 | 0.0066 | 0.0139 | 0.0048 | 0.0155 | 0.0038    | 0.0451 | 0.5109829377062080  | 1.2486141634231800 |
| CVAE  | mind | 300 | 10       | 0.001    | 0.5340 | 0.4754 | 0.6895 | 0.6592 | 0.0080 | 0.0087 | 0.0190 | 0.0070 | 0.0188 | 0.0045    | 0.0526 | 0.1794472347489970  | 1.382627534954400  |
| CVAE  | mind | 300 | 10       | 0.002    | 0.5340 | 0.4754 | 0.6895 | 0.6592 | 0.0080 | 0.0087 | 0.0190 | 0.0070 | 0.0188 | 0.0045    | 0.0526 | 0.1794472347489970  | 1.244074395302720  |
| CVAE  | mind | 300 | 10       | 0.005    | 0.5340 | 0.4754 | 0.6895 | 0.6592 | 0.0080 | 0.0087 | 0.0190 | 0.0070 | 0.0188 | 0.0045    | 0.0526 | 0.1794472347489970  | 1.4521510802250600 |
| CVAE  | mind | 300 | 10       | 0.01     | 0.5340 | 0.4754 | 0.6895 | 0.6592 | 0.0080 | 0.0087 | 0.0190 | 0.0070 | 0.0188 | 0.0045    | 0.0526 | 0.1794472347489970  | 1.3797458906460800 |
| CVAE  | mind | 400 | 10       | 0.001    | 0.5376 | 0.4815 | 0.6939 | 0.6553 | 0.0076 | 0.0081 | 0.0155 | 0.0051 | 0.0180 | 0.0043    | 0.0566 | 0.29419397487419200 | 1.3792860472459100 |
| CVAE  | mind | 400 | 10       | 0.002    | 0.5376 | 0.4815 | 0.6939 | 0.6553 | 0.0076 | 0.0081 | 0.0155 | 0.0051 | 0.0180 | 0.0043    | 0.0566 | 0.29419397487419200 | 1.5249004860123400 |
| CVAE  | mind | 400 | 10       | 0.005    | 0.5376 | 0.4815 | 0.6939 | 0.6553 | 0.0076 | 0.0081 | 0.0155 | 0.0051 | 0.0180 | 0.0043    | 0.0566 | 0.29419397487419200 | 1.556164814630390  |
| CVAE  | mind | 400 | 10       | 0.01     | 0.5376 | 0.4815 | 0.6939 | 0.6553 | 0.0076 | 0.0081 | 0.0155 | 0.0051 | 0.0180 | 0.0043    | 0.0566 | 0.29419397487419200 | 1.55489792032324   |
| CVAE  | movielens100k | 10  | 10       | 1        | 2.5402 | 7.7245 | 2.7793 | 0.6605 | 0.0399 | 0.0350 | 0.1326 | 0.0490 | 0.0636 | 0.0313    | 0.0933 | 0.07442184796745930  |
| CVAE  | movielens100k | 10  | 10       | 2        | 2.5402 | 7.7245 | 2.7793 | 0.6605 | 0.0399 | 0.0350 | 0.1326 | 0.0490 | 0.0636 | 0.0313    | 0.0933 | 0.07442184796745930  |
| CVAE  | movielens100k | 10  | 10       | 5        | 2.5402 | 7.7245 | 2.7793 | 0.6605 | 0.0399 | 0.0350 | 0.1326 | 0.0490 | 0.0636 | 0.0313    | 0.0933 | 0.07442184796745930  |
| CVAE  | movielens100k | 10  | 10       | 10       | 2.5402 | 7.7245 | 2.7793 | 0.6605 | 0.0399 | 0.0350 | 0.1326 | 0.0490 | 0.0636 | 0.0313    | 0.0933 | 0.07442184796745930  |
| CVAE  | movielens100k | 50  | 10       | 1        | 2.5402 | 7.7245 | 2.7793 | 0.7642 | 0.0551 | 0.0445 | 0.1327 | 0.0550 | 0.0811 | 0.0429    | 0.1245 | 0.05082567285749090  |
| CVAE  | movielens100k | 50  | 10       | 2        | 2.5402 | 7.7245 | 2.7793 | 0.7642 | 0.0551 | 0.0445 | 0.1327 | 0.0550 | 0.0811 | 0.0429    | 0.1245 | 0.05082567285749090  |
| CVAE  | movielens100k | 50  | 10       | 5        | 2.5402 | 7.7245 | 2.7793 | 0.7642 | 0.0551 | 0.0445 | 0.1327 | 0.0550 | 0.0811 | 0.0429    | 0.1245 | 0.05082567285749090  |
| CVAE  | movielens100k | 50  | 10       | 10       | 2.5402 | 7.7245 | 2.7793 | 0.7642 | 0.0551 | 0.0445 | 0.1327 | 0.0550 | 0.0811 | 0.0429    | 0.1245 | 0.05082567285749090  |
| CVAE  | movielens100k | 100 | 10       | 1        | 2.5402 | 7.7245 | 2.7793 | 0.8141 | 0.0645 | 0.0539 | 0.1464 | 0.0628 | 0.0984 | 0.0486    | 0.1619 | 0.043174960968865100 |
| CVAE  | movielens100k | 100 | 10       | 2        | 2.5402 | 7.7245 | 2.7793 | 0.8141 | 0.0645 | 0.0539 | 0.1464 | 0.0628 | 0.0984 | 0.0486    | 0.1619 | 0.043174960968865100 |
| CVAE  | movielens100k | 100 | 10       | 5        | 2.5402 | 7.7245 | 2.7793 | 0.8141 | 0.0645 | 0.0539 | 0.1464 | 0.0628 | 0.0984 | 0.0486    | 0.1619 | 0.043174960968865100 |
| CVAE  | movielens100k | 100 | 10       | 10       | 2.5402 | 7.7245 | 2.7793 | 0.8141 | 0.0645 | 0.0539 | 0.1464 | 0.0628 | 0.0984 | 0.0486    | 0.1619 | 0.043174960968865100 |
| CVAE  | movielens100k | 200 | 10       | 1        | 2.5402 | 7.7245 | 2.7793 | 0.8432 | 0.0711 | 0.0577 | 0.1445 | 0.0634 | 0.1049 | 0.0533    | 0.1797 | 0.05120007167806990  |
| CVAE  | movielens100k | 200 | 10       | 2        | 2.5402 | 7.7245 | 2.7793 | 0.8432 | 0.0711 | 0.0577 | 0.1445 | 0.0634 | 0.1049 | 0.0533    | 0.1797 | 0.05120007167806990  |
| CVAE  | movielens100k | 200 | 10       | 5        | 2.5402 | 7.7245 | 2.7793 | 0.8432 | 0.0711 | 0.0577 | 0.1445 | 0.0634 | 0.1049 | 0.0533    | 0.1797 | 0.05120007167806990  |
| CVAE  | movielens100k | 200 | 10       | 10       | 2.5402 | 7.7245 | 2.7793 | 0.8432 | 0.0711 | 0.0577 | 0.1445 | 0.0634 | 0.1049 | 0.0533    | 0.1797 | 0.05120007167806990  |
| CVAE  | movielens100k | 300 | 10       | 1        | 2.4838 | 7.4118 | 2.7225 | 0.8600 | 0.0837 | 0.0625 | 0.1270 | 0.0644 | 0.1210 | 0.0624    | 0.2201 | 0.11868722077085500  |
| CVAE  | movielens100k | 300 | 10       | 2        | 2.4838 | 7.4118 | 2.7225 | 0.8600 | 0.0837 | 0.0625 | 0.1270 | 0.0644 | 0.1210 | 0.0624    | 0.2201 | 0.11868722077085500  |
| CVAE  | movielens100k | 300 | 10       | 5        | 2.4838 | 7.4118 | 2.7225 | 0.8600 | 0.0837 | 0.0625 | 0.1270 | 0.0644 | 0.1210 | 0.0624    | 0.2201 | 0.11868722077085500  |
| CVAE  | movielens100k | 300 | 10       | 10       | 2.4838 | 7.4118 | 2.7225 | 0.8600 | 0.0837 | 0.0625 | 0.1270 | 0.0644 | 0.1210 | 0.0624    | 0.2201 | 0.11868722077085500  |
| CVAE  | movielens100k | 400 | 10       | 1        | 2.2683 | 6.2950 | 2.5090 | 0.8655 | 0.0841 | 0.0641 | 0.1351 | 0.0660 | 0.1218 | 0.0624    | 0.2223 | 0.12140655888798800  |
| CVAE  | movielens100k | 400 | 10       | 2        | 2.2683 | 6.2950 | 2.5090 | 0.8655 | 0.0841 | 0.0641 | 0.1351 | 0.0660 | 0.1218 | 0.0624    | 0.2223 | 0.12140655888798800  |
| CVAE  | movielens100k | 400 | 10       | 5        | 2.2683 | 6.2950 | 2.5090 | 0.8655 | 0.0841 | 0.0641 | 0.1351 | 0.0660 | 0.1218 | 0.0624    | 0.2223 | 0.12140655888798800  |
| CVAE  | movielens100k | 400 | 10       | 10       | 2.2683 | 6.2950 | 2.5090 | 0.8655 | 0.0841 | 0.0641 | 0.1351 | 0.0660 | 0.1218 | 0.0624    | 0.2223 | 0.12140655888798800  |

### Result analysis
From the above table, we observed that recalled value is the largest when k=50, and lambda=0.001. So we increase the number of iterations to 50 and re-calculate the accuracy and got a higher accuracy value. Here is the parameters we chose:(k=50, vae_layers=[200, 100], act_fn="sigmoid", input_dim=8000, lr=0.001, batch_size=128, max_iter=100,lambda_u=1e-4, lambda_v=0.001, lambda_r=10, lambda_w=1e-4, seed=123, verbose=True)

The following are the accuracy values using the most optimized parameters on the different datasets:
- amazon clothing dataset
![image](uploads/9ba2d967255cb105ae0408df67a1a2a7/image.png)
- amazon digital music dataset
![image](uploads/cc30c09e5f9ea04f91a38ed287a97f28/image.png)
- citeulike dataset
![image](uploads/361fa8bd510fafc7f60dd85b0488f5f5/image.png)
- movienlens 10k dataset
![image](uploads/425b3b42c598e7b7edfe87758fa2d923/image.png)
- mind dataset
![image](uploads/044b076fe221bd71541e135996670d9e/image.png)

From the above five figures, we can see that the CVAE model performs the best on the MovieLens and the Amazon digital music datasets, with a recall value of about 0.44.

TODO:
- [ ] is the recall value the same for both datasets?

Also, we observed that for category diversity, the MIND dataset shows a better diversity score than the MovieLens dataset, with a diversity value of around 0.2. 